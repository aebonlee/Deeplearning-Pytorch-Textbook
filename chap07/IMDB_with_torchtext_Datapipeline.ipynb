{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchtext torchdata spacy\n",
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torchdata.datapipes.iter import FileOpener, HttpReader, IterableWrapper, MapToIterConverter, Demultiplexer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "#from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils Included\n",
    "# The following utility functions are copied from torchtext\n",
    "# https://github.com/pytorch/text/blob/main/torchtext/data/datasets_utils.py\n",
    "import functools\n",
    "import inspect\n",
    "import os\n",
    "\n",
    "\n",
    "def _check_default_set(split, target_select, dataset_name):\n",
    "    # Check whether given object split is either a tuple of strings or string\n",
    "    # and represents a valid selection of options given by the tuple of strings\n",
    "    # target_select.\n",
    "    if isinstance(split, str):\n",
    "        split = (split,)\n",
    "    if isinstance(target_select, str):\n",
    "        target_select = (target_select,)\n",
    "    if not isinstance(split, tuple):\n",
    "        raise ValueError(\"Internal error: Expected split to be of type tuple.\")\n",
    "    if not set(split).issubset(set(target_select)):\n",
    "        raise TypeError(\n",
    "            \"Given selection {} of splits is not supported for dataset {}. Please choose from {}.\".format(\n",
    "                split, dataset_name, target_select\n",
    "            )\n",
    "        )\n",
    "    return split\n",
    "\n",
    "\n",
    "def _wrap_datasets(datasets, split):\n",
    "    # Wrap return value for _setup_datasets functions to support singular values instead\n",
    "    # of tuples when split is a string.\n",
    "    if isinstance(split, str):\n",
    "        if len(datasets) != 1:\n",
    "            raise ValueError(\"Internal error: Expected number of datasets is not 1.\")\n",
    "        return datasets[0]\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def _dataset_docstring_header(fn, num_lines=None, num_classes=None):\n",
    "    \"\"\"\n",
    "    Returns docstring for a dataset based on function arguments.\n",
    "    Assumes function signature of form (root='.data', split=<some tuple of strings>, **kwargs)\n",
    "    \"\"\"\n",
    "    argspec = inspect.getfullargspec(fn)\n",
    "    if not (argspec.args[0] == \"root\" and argspec.args[1] == \"split\"):\n",
    "        raise ValueError(\n",
    "            f\"Internal Error: Given function {fn} did not adhere to standard signature.\"\n",
    "        )\n",
    "    default_split = argspec.defaults[1]\n",
    "\n",
    "    if not (isinstance(default_split, tuple) or isinstance(default_split, str)):\n",
    "        raise ValueError(\n",
    "            f\"default_split type expected to be of string or tuple but got {type(default_split)}\"\n",
    "        )\n",
    "\n",
    "    header_s = fn.__name__ + \" dataset\\n\"\n",
    "\n",
    "    if isinstance(default_split, tuple):\n",
    "        header_s += \"\\nSeparately returns the {} split\".format(\"/\".join(default_split))\n",
    "\n",
    "    if isinstance(default_split, str):\n",
    "        header_s += f\"\\nOnly returns the {default_split} split\"\n",
    "\n",
    "    if num_lines is not None:\n",
    "        header_s += \"\\n\\nNumber of lines per split:\"\n",
    "        for k, v in num_lines.items():\n",
    "            header_s += f\"\\n    {k}: {v}\\n\"\n",
    "\n",
    "    if num_classes is not None:\n",
    "        header_s += \"\\n\\nNumber of classes\"\n",
    "        header_s += f\"\\n    {num_classes}\\n\"\n",
    "\n",
    "    args_s = \"\\nArgs:\"\n",
    "    args_s += \"\\n    root: Directory where the datasets are saved.\"\n",
    "    args_s += \"\\n        Default: .data\"\n",
    "\n",
    "    if isinstance(default_split, tuple):\n",
    "        args_s += \"\\n    split: split or splits to be returned. Can be a string or tuple of strings.\"\n",
    "        args_s += \"\\n        Default: {}\" \"\".format(str(default_split))\n",
    "\n",
    "    if isinstance(default_split, str):\n",
    "        args_s += \"\\n     split: Only {default_split} is available.\"\n",
    "        args_s += (\n",
    "            \"\\n         Default: {default_split}.format(default_split=default_split)\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join([header_s, args_s]) + \"\\n\"\n",
    "\n",
    "\n",
    "def _add_docstring_header(docstring=None, num_lines=None, num_classes=None):\n",
    "    def docstring_decorator(fn):\n",
    "        old_doc = fn.__doc__\n",
    "        fn.__doc__ = _dataset_docstring_header(fn, num_lines, num_classes)\n",
    "        if docstring is not None:\n",
    "            fn.__doc__ += docstring\n",
    "        if old_doc is not None:\n",
    "            fn.__doc__ += old_doc\n",
    "        return fn\n",
    "\n",
    "    return docstring_decorator\n",
    "\n",
    "\n",
    "def _wrap_split_argument_with_fn(fn, splits):\n",
    "    \"\"\"\n",
    "    Wraps given function of specific signature to extend behavior of split\n",
    "    to support individual strings. The given function is expected to have a split\n",
    "    kwarg that accepts tuples of strings, e.g. ('train', 'valid') and the returned\n",
    "    function will have a split argument that also accepts strings, e.g. 'train', which\n",
    "    are then turned single entry tuples. Furthermore, the return value of the wrapped\n",
    "    function is unpacked if split is only a single string to enable behavior such as\n",
    "    train = AG_NEWS(split='train')\n",
    "    train, valid = AG_NEWS(split=('train', 'valid'))\n",
    "    \"\"\"\n",
    "    argspec = inspect.getfullargspec(fn)\n",
    "    if not (\n",
    "        argspec.args[0] == \"root\"\n",
    "        and argspec.args[1] == \"split\"\n",
    "        and argspec.varargs is None\n",
    "        and argspec.varkw is None\n",
    "        and len(argspec.kwonlyargs) == 0\n",
    "        and len(argspec.annotations) == 0\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Internal Error: Given function {fn} did not adhere to standard signature.\"\n",
    "        )\n",
    "\n",
    "    @functools.wraps(fn)\n",
    "    def new_fn(root=os.path.expanduser(\"~/.torchtext/cache\"), split=splits, **kwargs):\n",
    "        result = []\n",
    "        for item in _check_default_set(split, splits, fn.__name__):\n",
    "            result.append(fn(root, item, **kwargs))\n",
    "        return _wrap_datasets(tuple(result), split)\n",
    "\n",
    "    new_sig = inspect.signature(new_fn)\n",
    "    new_sig_params = new_sig.parameters\n",
    "    new_params = []\n",
    "    new_params.append(new_sig_params[\"root\"].replace(default=\".data\"))\n",
    "    new_params.append(new_sig_params[\"split\"].replace(default=splits))\n",
    "    new_params += [entry[1] for entry in list(new_sig_params.items())[2:]]\n",
    "    new_sig = new_sig.replace(parameters=tuple(new_params))\n",
    "    new_fn.__signature__ = new_sig\n",
    "\n",
    "    return new_fn\n",
    "\n",
    "\n",
    "def _wrap_split_argument(splits):\n",
    "    def new_fn(fn):\n",
    "        return _wrap_split_argument_with_fn(fn, splits)\n",
    "\n",
    "    return new_fn\n",
    "\n",
    "\n",
    "def _create_dataset_directory(dataset_name):\n",
    "    def decorator(func):\n",
    "        argspec = inspect.getfullargspec(func)\n",
    "        if not (\n",
    "            argspec.args[0] == \"root\"\n",
    "            and argspec.args[1] == \"split\"\n",
    "            and argspec.varargs is None\n",
    "            and argspec.varkw is None\n",
    "            and len(argspec.kwonlyargs) == 0\n",
    "            and len(argspec.annotations) == 0\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"Internal Error: Given function {func} did not adhere to standard signature.\"\n",
    "            )\n",
    "\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(root=os.path.expanduser(\"~/.torchtext/cache\"), *args, **kwargs):\n",
    "            new_root = os.path.join(root, dataset_name)\n",
    "            if not os.path.exists(new_root):\n",
    "                os.makedirs(new_root)\n",
    "            return func(root=new_root, *args, **kwargs)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading IMDB Pipeline From https://github.com/pytorch/data/blob/main/examples/text/imdb.py\n",
    "URL = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "MD5 = \"7c2ac02c03563afcf9b574c7e56c153a\"\n",
    "\n",
    "NUM_LINES = {\n",
    "    \"train\": 25000,\n",
    "    \"test\": 25000,\n",
    "}\n",
    "\n",
    "_PATH = \"aclImdb_v1.tar.gz\"\n",
    "\n",
    "DATASET_NAME = \"IMDB\"\n",
    "\n",
    "\n",
    "def _path_fn(root, path):\n",
    "    return os.path.join(root, os.path.basename(path))\n",
    "\n",
    "\n",
    "def _filter_fn(split, t):\n",
    "    return Path(t[0]).parts[-3] == split and Path(t[0]).parts[-2] in [\"pos\", \"neg\"]\n",
    "\n",
    "\n",
    "def _file_to_sample(t):\n",
    "    return Path(t[0]).parts[-2], t[1].read().decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@_add_docstring_header(num_lines=NUM_LINES, num_classes=2)\n",
    "@_create_dataset_directory(dataset_name=DATASET_NAME)\n",
    "@_wrap_split_argument((\"train\", \"test\"))\n",
    "def IMDB(root, split):\n",
    "    \"\"\"Demonstrates complex use case where each sample is stored in separate file and compressed in tar file\n",
    "    Here we show some fancy filtering and mapping operations.\n",
    "    Filtering is needed to know which files belong to train/test and neg/pos label\n",
    "    Mapping is needed to yield proper data samples by extracting label from file name\n",
    "        and reading data from file\n",
    "    \"\"\"\n",
    "\n",
    "    url_dp = IterableWrapper([URL])\n",
    "    # cache data on-disk\n",
    "    cache_dp = url_dp.on_disk_cache(\n",
    "        filepath_fn=partial(_path_fn, root),\n",
    "        hash_dict={_path_fn(root, URL): MD5},\n",
    "        hash_type=\"md5\",\n",
    "    )\n",
    "    cache_dp = HttpReader(cache_dp).end_caching(mode=\"wb\", same_filepath_fn=True)\n",
    "\n",
    "    cache_dp = FileOpener(cache_dp, mode=\"b\")\n",
    "\n",
    "    # stack TAR extractor on top of load files data pipe\n",
    "    extracted_files = cache_dp.load_from_tar()\n",
    "\n",
    "    # filter the files as applicable to create dataset for given split (train or test)\n",
    "    filter_files = extracted_files.filter(partial(_filter_fn, split))\n",
    "\n",
    "    # map the file to yield proper data samples\n",
    "    sample = filter_files.map(_file_to_sample)\n",
    "\n",
    "    def convlabel(x):\n",
    "        r = None\n",
    "        if x[0] == 'pos':\n",
    "            r = 1\n",
    "        elif x[0] == 'neg':\n",
    "            r = 0\n",
    "        else:\n",
    "            r = -1\n",
    "            raise (ValueError(f'Error: {x[0]} is not proper value'))\n",
    "\n",
    "        return r, x[1]\n",
    "\n",
    "    # sample = sample.map(lambda x: (1 if x[0]=='pos' else 0, x[1]))\n",
    "    sample = sample.map(convlabel)\n",
    "    # sample = sample.shuffle().set_shuffle(False).sharding_filter()\n",
    "    sample = sample.shuffle()\n",
    "    sample = sample.shuffle().sharding_filter()\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161, 8, 42, 490]\n"
     ]
    }
   ],
   "source": [
    "train_iter = IMDB(split='train')\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = get_tokenizer('spacy', 'en_core_web_md')\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield (tokenizer(text))\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "\n",
    "print(vocab(['here', 'is', 'an', 'example']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "SPLIT_SIZE = 0.8\n",
    "LR = 0.001\n",
    "MAX_EPOCHS = 15\n",
    "vocab_size = len(vocab)\n",
    "n_classes = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRNN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2\n",
    "    ):\n",
    "        super(BasicRNN, self).__init__()\n",
    "        self.n_layers = n_layers  # ------ RNN 계층에 대한 개수\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)  # ------ 워드 임베딩 적용\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout_p)  # ------ 드롭아웃 적용\n",
    "        self.rnn = nn.RNN(\n",
    "            embed_dim, self.hidden_dim, num_layers=self.n_layers, batch_first=True\n",
    "        )\n",
    "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  # ------ 문자를 숫자/벡터로 변환\n",
    "        h_0 = self._init_state(batch_size=x.size(0))  # ------ 최초 은닉 상태의 값을 0으로 초기화\n",
    "        x, _ = self.rnn(x, h_0)  # ------ RNN 계층을 의미하며, 파라미터로 입력과 이전 은닉 상태의 값을 받습니다.\n",
    "        h_t = x[:, -1, :]  # ------ 모든 네트워크를 거쳐서 가장 마지막에 나온 단어의 임베딩 값(마지막 은닉 상태의 값)\n",
    "        self.dropout(h_t)\n",
    "        logit = torch.sigmoid(self.out(h_t))\n",
    "        return logit\n",
    "\n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data  # ------ 모델의 파라미터 값을 가져와서 weight 변수에 저장\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, batch_idx, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, batch_idx, \"test\")\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        label, text = self._prepare_batch(batch)\n",
    "        return self(text)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=LR)\n",
    "        return optimizer\n",
    "\n",
    "    def _prepare_batch(self, batch):\n",
    "        label, text = batch\n",
    "        return label, text\n",
    "\n",
    "    def _common_step(self, batch, batch_idx, stage: str):\n",
    "        # x = self._prepare_batch(batch)\n",
    "        label, text = batch\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(self(text), label)\n",
    "        self.log(f\"{stage}_loss\", loss, on_step=True)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "_ChildDataPipe _ChildDataPipe\n"
     ]
    }
   ],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device)\n",
    "\n",
    "count_data = 0\n",
    "def spliter(n):\n",
    "    print(n)\n",
    "    if count_data < int(NUM_LINES['train'] * SPLIT_SIZE):\n",
    "        count_data += 1\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Refill Generators & Put in the DataLoader\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "split_train_, split_valid_ = train_iter.demux(classifier_fn=spliter, num_instances=2, buffer_size=NUM_LINES['train'])\n",
    "print(count_data)\n",
    "print(split_train_, split_valid_)\n",
    "\n",
    "#num_train = int(len(train_dataset) * SPLIT_SIZE)\n",
    "\n",
    "# split_train_, split_valid_ = random_split(\n",
    "#     train_dataset, [num_train, len(train_dataset) - num_train]\n",
    "# )\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=8,\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=8,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_iter,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=8,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type      | Params\n",
      "--------------------------------------\n",
      "0 | embed   | Embedding | 15.5 M\n",
      "1 | dropout | Dropout   | 0     \n",
      "2 | rnn     | RNN       | 98.8 K\n",
      "3 | out     | Linear    | 514   \n",
      "--------------------------------------\n",
      "15.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.6 M    Total params\n",
      "62.383    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fef7398053c440cade8e68a1c048b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 13496, 19524, 5712, 17624, 9104, 32224, 31296, 32360) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1164\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Timepercent\\AI Learning\\Deeplearning-Pytorch-Textbook\\chap07\\IMDB_with_torchtext_Datapipeline.ipynb 셀 10\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/IMDB_with_torchtext_Datapipeline.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/IMDB_with_torchtext_Datapipeline.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39mMAX_EPOCHS)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/IMDB_with_torchtext_Datapipeline.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/IMDB_with_torchtext_Datapipeline.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel, train_dataloaders\u001b[39m=\u001b[39;49mtrain_dataloader, val_dataloaders\u001b[39m=\u001b[39;49mvalid_dataloader,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/IMDB_with_torchtext_Datapipeline.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/IMDB_with_torchtext_Datapipeline.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m trainer\u001b[39m.\u001b[39mtest(model, dataloaders\u001b[39m=\u001b[39mtest_dataloader)\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:696\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 696\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    697\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    698\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    649\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    651\u001b[0m \u001b[39m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:737\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    733\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    734\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[0;32m    735\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    736\u001b[0m )\n\u001b[1;32m--> 737\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[0;32m    739\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    740\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1168\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1166\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m-> 1168\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m   1170\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1254\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   1253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1254\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1276\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[0;32m   1275\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1276\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[0;32m   1278\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1345\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1343\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m   1344\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m-> 1345\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1347\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1349\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py:155\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    154\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[1;32m--> 155\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[0;32m    157\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:195\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_skip()\n\u001b[0;32m    193\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[1;32m--> 195\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_run_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    197\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[0;32m    198\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py:88\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.on_run_start\u001b[1;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reload_dataloader_state_dict(data_fetcher)\n\u001b[0;32m     87\u001b[0m \u001b[39m# creates the iterator inside the fetcher but returns `self`\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(data_fetcher)\n\u001b[0;32m     89\u001b[0m \u001b[39m# add the previous `fetched` value to properly track `is_last_batch` with no prefetching\u001b[39;00m\n\u001b[0;32m     90\u001b[0m data_fetcher\u001b[39m.\u001b[39mfetched \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mcurrent\u001b[39m.\u001b[39mready\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:181\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader)\n\u001b[0;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_patch()\n\u001b[1;32m--> 181\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprefetching()\n\u001b[0;32m    182\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:242\u001b[0m, in \u001b[0;36mDataFetcher.prefetching\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetch_batches):\n\u001b[0;32m    241\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 242\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch(iterator)\n\u001b[0;32m    243\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m         \u001b[39m# this would only happen when prefetch_batches > the number of batches available and makes\u001b[39;00m\n\u001b[0;32m    245\u001b[0m         \u001b[39m# `fetching_function` jump directly to the empty iterator case without trying to fetch again\u001b[39;00m\n\u001b[0;32m    246\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:278\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fetch_next_batch\u001b[39m(\u001b[39mself\u001b[39m, iterator: Iterator) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m     start_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_fetch_start()\n\u001b[1;32m--> 278\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[0;32m    279\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfetched \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetch_batches \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_len:\n\u001b[0;32m    281\u001b[0m         \u001b[39m# when we don't prefetch but the dataloader is sized, we use the length for `done`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1358\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1359\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1361\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1362\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1324\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m-> 1325\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1326\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1327\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1176\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1175\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1176\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[0;32m   1178\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 13496, 19524, 5712, 17624, 9104, 32224, 31296, 32360) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "model = BasicRNN(\n",
    "    n_layers=1,\n",
    "    hidden_dim=256,\n",
    "    n_vocab=vocab_size,\n",
    "    embed_dim=128,\n",
    "    n_classes=n_classes,\n",
    "    dropout_p=0.5,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    trainer = pl.Trainer(max_epochs=MAX_EPOCHS, accelerator='gpu', devices=1)\n",
    "else:\n",
    "    trainer = pl.Trainer(max_epochs=MAX_EPOCHS)\n",
    "\n",
    "trainer.fit(\n",
    "    model=model, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader,\n",
    ")\n",
    "trainer.test(model, dataloaders=test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e45d430dd4495a451adf3f96c36ab39ddd21d42ca8131a0e0d50ee113b976380"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
