{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torchdata.datapipes.iter import FileOpener, HttpReader, IterableWrapper\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils Included\n",
    "# The following utility functions are copied from torchtext\n",
    "# https://github.com/pytorch/text/blob/main/torchtext/data/datasets_utils.py\n",
    "import functools\n",
    "import inspect\n",
    "import os\n",
    "\n",
    "def _check_default_set(split, target_select, dataset_name):\n",
    "    # Check whether given object split is either a tuple of strings or string\n",
    "    # and represents a valid selection of options given by the tuple of strings\n",
    "    # target_select.\n",
    "    if isinstance(split, str):\n",
    "        split = (split,)\n",
    "    if isinstance(target_select, str):\n",
    "        target_select = (target_select,)\n",
    "    if not isinstance(split, tuple):\n",
    "        raise ValueError(\"Internal error: Expected split to be of type tuple.\")\n",
    "    if not set(split).issubset(set(target_select)):\n",
    "        raise TypeError(\n",
    "            \"Given selection {} of splits is not supported for dataset {}. Please choose from {}.\".format(\n",
    "                split, dataset_name, target_select\n",
    "            )\n",
    "        )\n",
    "    return split\n",
    "\n",
    "\n",
    "def _wrap_datasets(datasets, split):\n",
    "    # Wrap return value for _setup_datasets functions to support singular values instead\n",
    "    # of tuples when split is a string.\n",
    "    if isinstance(split, str):\n",
    "        if len(datasets) != 1:\n",
    "            raise ValueError(\"Internal error: Expected number of datasets is not 1.\")\n",
    "        return datasets[0]\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def _dataset_docstring_header(fn, num_lines=None, num_classes=None):\n",
    "    \"\"\"\n",
    "    Returns docstring for a dataset based on function arguments.\n",
    "    Assumes function signature of form (root='.data', split=<some tuple of strings>, **kwargs)\n",
    "    \"\"\"\n",
    "    argspec = inspect.getfullargspec(fn)\n",
    "    if not (argspec.args[0] == \"root\" and argspec.args[1] == \"split\"):\n",
    "        raise ValueError(f\"Internal Error: Given function {fn} did not adhere to standard signature.\")\n",
    "    default_split = argspec.defaults[1]\n",
    "\n",
    "    if not (isinstance(default_split, tuple) or isinstance(default_split, str)):\n",
    "        raise ValueError(f\"default_split type expected to be of string or tuple but got {type(default_split)}\")\n",
    "\n",
    "    header_s = fn.__name__ + \" dataset\\n\"\n",
    "\n",
    "    if isinstance(default_split, tuple):\n",
    "        header_s += \"\\nSeparately returns the {} split\".format(\"/\".join(default_split))\n",
    "\n",
    "    if isinstance(default_split, str):\n",
    "        header_s += f\"\\nOnly returns the {default_split} split\"\n",
    "\n",
    "    if num_lines is not None:\n",
    "        header_s += \"\\n\\nNumber of lines per split:\"\n",
    "        for k, v in num_lines.items():\n",
    "            header_s += f\"\\n    {k}: {v}\\n\"\n",
    "\n",
    "    if num_classes is not None:\n",
    "        header_s += \"\\n\\nNumber of classes\"\n",
    "        header_s += f\"\\n    {num_classes}\\n\"\n",
    "\n",
    "    args_s = \"\\nArgs:\"\n",
    "    args_s += \"\\n    root: Directory where the datasets are saved.\"\n",
    "    args_s += \"\\n        Default: .data\"\n",
    "\n",
    "    if isinstance(default_split, tuple):\n",
    "        args_s += \"\\n    split: split or splits to be returned. Can be a string or tuple of strings.\"\n",
    "        args_s += \"\\n        Default: {}\" \"\".format(str(default_split))\n",
    "\n",
    "    if isinstance(default_split, str):\n",
    "        args_s += \"\\n     split: Only {default_split} is available.\"\n",
    "        args_s += \"\\n         Default: {default_split}.format(default_split=default_split)\"\n",
    "\n",
    "    return \"\\n\".join([header_s, args_s]) + \"\\n\"\n",
    "\n",
    "\n",
    "def _add_docstring_header(docstring=None, num_lines=None, num_classes=None):\n",
    "    def docstring_decorator(fn):\n",
    "        old_doc = fn.__doc__\n",
    "        fn.__doc__ = _dataset_docstring_header(fn, num_lines, num_classes)\n",
    "        if docstring is not None:\n",
    "            fn.__doc__ += docstring\n",
    "        if old_doc is not None:\n",
    "            fn.__doc__ += old_doc\n",
    "        return fn\n",
    "\n",
    "    return docstring_decorator\n",
    "\n",
    "\n",
    "def _wrap_split_argument_with_fn(fn, splits):\n",
    "    \"\"\"\n",
    "    Wraps given function of specific signature to extend behavior of split\n",
    "    to support individual strings. The given function is expected to have a split\n",
    "    kwarg that accepts tuples of strings, e.g. ('train', 'valid') and the returned\n",
    "    function will have a split argument that also accepts strings, e.g. 'train', which\n",
    "    are then turned single entry tuples. Furthermore, the return value of the wrapped\n",
    "    function is unpacked if split is only a single string to enable behavior such as\n",
    "    train = AG_NEWS(split='train')\n",
    "    train, valid = AG_NEWS(split=('train', 'valid'))\n",
    "    \"\"\"\n",
    "    argspec = inspect.getfullargspec(fn)\n",
    "    if not (\n",
    "        argspec.args[0] == \"root\"\n",
    "        and argspec.args[1] == \"split\"\n",
    "        and argspec.varargs is None\n",
    "        and argspec.varkw is None\n",
    "        and len(argspec.kwonlyargs) == 0\n",
    "        and len(argspec.annotations) == 0\n",
    "    ):\n",
    "        raise ValueError(f\"Internal Error: Given function {fn} did not adhere to standard signature.\")\n",
    "\n",
    "    @functools.wraps(fn)\n",
    "    def new_fn(root=os.path.expanduser(\"~/.torchtext/cache\"), split=splits, **kwargs):\n",
    "        result = []\n",
    "        for item in _check_default_set(split, splits, fn.__name__):\n",
    "            result.append(fn(root, item, **kwargs))\n",
    "        return _wrap_datasets(tuple(result), split)\n",
    "\n",
    "    new_sig = inspect.signature(new_fn)\n",
    "    new_sig_params = new_sig.parameters\n",
    "    new_params = []\n",
    "    new_params.append(new_sig_params[\"root\"].replace(default=\".data\"))\n",
    "    new_params.append(new_sig_params[\"split\"].replace(default=splits))\n",
    "    new_params += [entry[1] for entry in list(new_sig_params.items())[2:]]\n",
    "    new_sig = new_sig.replace(parameters=tuple(new_params))\n",
    "    new_fn.__signature__ = new_sig\n",
    "\n",
    "    return new_fn\n",
    "\n",
    "\n",
    "def _wrap_split_argument(splits):\n",
    "    def new_fn(fn):\n",
    "        return _wrap_split_argument_with_fn(fn, splits)\n",
    "\n",
    "    return new_fn\n",
    "\n",
    "\n",
    "def _create_dataset_directory(dataset_name):\n",
    "    def decorator(func):\n",
    "        argspec = inspect.getfullargspec(func)\n",
    "        if not (\n",
    "            argspec.args[0] == \"root\"\n",
    "            and argspec.args[1] == \"split\"\n",
    "            and argspec.varargs is None\n",
    "            and argspec.varkw is None\n",
    "            and len(argspec.kwonlyargs) == 0\n",
    "            and len(argspec.annotations) == 0\n",
    "        ):\n",
    "            raise ValueError(f\"Internal Error: Given function {func} did not adhere to standard signature.\")\n",
    "\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(root=os.path.expanduser(\"~/.torchtext/cache\"), *args, **kwargs):\n",
    "            new_root = os.path.join(root, dataset_name)\n",
    "            if not os.path.exists(new_root):\n",
    "                os.makedirs(new_root)\n",
    "            return func(root=new_root, *args, **kwargs)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading IMDB Pipeline From https://github.com/pytorch/data/blob/main/examples/text/imdb.py\n",
    "URL = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "MD5 = \"7c2ac02c03563afcf9b574c7e56c153a\"\n",
    "\n",
    "NUM_LINES = {\n",
    "    \"train\": 25000,\n",
    "    \"test\": 25000,\n",
    "}\n",
    "\n",
    "_PATH = \"aclImdb_v1.tar.gz\"\n",
    "\n",
    "DATASET_NAME = \"IMDB\"\n",
    "\n",
    "def _path_fn(root, path):\n",
    "    return os.path.join(root, os.path.basename(path))\n",
    "\n",
    "\n",
    "def _filter_fn(split, t):\n",
    "    return Path(t[0]).parts[-3] == split and Path(t[0]).parts[-2] in [\"pos\", \"neg\"]\n",
    "\n",
    "\n",
    "def _file_to_sample(t):\n",
    "    return Path(t[0]).parts[-2], t[1].read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@_add_docstring_header(num_lines=NUM_LINES, num_classes=2)\n",
    "@_create_dataset_directory(dataset_name=DATASET_NAME)\n",
    "@_wrap_split_argument((\"train\", \"test\"))\n",
    "def IMDB(root, split):\n",
    "    \"\"\"Demonstrates complex use case where each sample is stored in separate file and compressed in tar file\n",
    "    Here we show some fancy filtering and mapping operations.\n",
    "    Filtering is needed to know which files belong to train/test and neg/pos label\n",
    "    Mapping is needed to yield proper data samples by extracting label from file name\n",
    "        and reading data from file\n",
    "    \"\"\"\n",
    "\n",
    "    url_dp = IterableWrapper([URL])\n",
    "    # cache data on-disk\n",
    "    cache_dp = url_dp.on_disk_cache(\n",
    "        filepath_fn=partial(_path_fn, root),\n",
    "        hash_dict={_path_fn(root, URL): MD5},\n",
    "        hash_type=\"md5\",\n",
    "    )\n",
    "    cache_dp = HttpReader(cache_dp).end_caching(mode=\"wb\", same_filepath_fn=True)\n",
    "\n",
    "    cache_dp = FileOpener(cache_dp, mode=\"b\")\n",
    "\n",
    "    # stack TAR extractor on top of load files data pipe\n",
    "    extracted_files = cache_dp.load_from_tar()\n",
    "\n",
    "    # filter the files as applicable to create dataset for given split (train or test)\n",
    "    filter_files = extracted_files.filter(partial(_filter_fn, split))\n",
    "\n",
    "    # map the file to yield proper data samples\n",
    "    sample = filter_files.map(_file_to_sample)\n",
    "    \n",
    "    def convlabel(x):\n",
    "        r = None\n",
    "        if x[0] == 'pos':\n",
    "            r = 1\n",
    "        elif x[0] == 'neg':\n",
    "            r = 0\n",
    "        else:\n",
    "            r = -1\n",
    "            raise(ValueError(f'Error: {x[0]} is not proper value'))\n",
    "        \n",
    "        return r, x[1]\n",
    "    \n",
    "    # sample = sample.map(lambda x: (1 if x[0]=='pos' else 0, x[1]))\n",
    "    sample = sample.map(convlabel)\n",
    "    # sample = sample.shuffle().set_shuffle(False).sharding_filter()\n",
    "    sample = sample.shuffle().sharding_filter()\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161, 8, 42, 490]\n"
     ]
    }
   ],
   "source": [
    "train_iter = IMDB(split='train')\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = get_tokenizer('spacy', 'en_core_web_md')\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield(tokenizer(text))\n",
    "        \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "\n",
    "print(vocab(['here', 'is', 'an', 'example']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "SPLIT_SIZE = 0.8\n",
    "LR = 0.001\n",
    "EPOCHS = 10\n",
    "vocab_size = len(vocab)\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRNN(pl.LightningModule):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
    "        super(BasicRNN, self).__init__()\n",
    "        self.n_layers = n_layers  # ------ RNN 계층에 대한 개수\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)  # ------ 워드 임베딩 적용\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout_p)  # ------ 드롭아웃 적용\n",
    "        self.rnn = nn.RNN(\n",
    "            embed_dim, self.hidden_dim, num_layers=self.n_layers, batch_first=True\n",
    "        )\n",
    "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  # ------ 문자를 숫자/벡터로 변환\n",
    "        h_0 = self._init_state(batch_size=x.size(0))  # ------ 최초 은닉 상태의 값을 0으로 초기화\n",
    "        x, _ = self.rnn(x, h_0)  # ------ RNN 계층을 의미하며, 파라미터로 입력과 이전 은닉 상태의 값을 받습니다.\n",
    "        h_t = x[:, -1, :]  # ------ 모든 네트워크를 거쳐서 가장 마지막에 나온 단어의 임베딩 값(마지막 은닉 상태의 값)\n",
    "        self.dropout(h_t)\n",
    "        logit = torch.sigmoid(self.out(h_t))\n",
    "        return logit\n",
    "\n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data  # ------ 모델의 파라미터 값을 가져와서 weight 변수에 저장\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, batch_idx, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, batch_idx, \"test\")\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        label, text = self._prepare_batch(batch)\n",
    "        return self(text)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=LR)\n",
    "        return optimizer\n",
    "\n",
    "    def _prepare_batch(self, batch):\n",
    "        label, text = batch\n",
    "        return label, text\n",
    "\n",
    "    def _common_step(self, batch, batch_idx, stage: str):\n",
    "        # x = self._prepare_batch(batch)\n",
    "        label, text = batch\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(self(text), label)\n",
    "        self.log(f\"{stage}_loss\", loss, on_step=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device)\n",
    "\n",
    "# Refill Generators & Put in the DataLoader\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * SPLIT_SIZE)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e45d430dd4495a451adf3f96c36ab39ddd21d42ca8131a0e0d50ee113b976380"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
