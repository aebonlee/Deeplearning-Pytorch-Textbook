{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchdata.datapipes.iter import FileOpener, HttpReader, IterableWrapper\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Utils\n",
    "from utils import _add_docstring_header, _create_dataset_directory, _wrap_split_argument\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading IMDB Pipeline From https://github.com/pytorch/data/blob/main/examples/text/imdb.py\n",
    "URL = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "MD5 = \"7c2ac02c03563afcf9b574c7e56c153a\"\n",
    "\n",
    "NUM_LINES = {\n",
    "    \"train\": 25000,\n",
    "    \"test\": 25000,\n",
    "}\n",
    "\n",
    "_PATH = \"aclImdb_v1.tar.gz\"\n",
    "\n",
    "DATASET_NAME = \"IMDB\"\n",
    "\n",
    "def _path_fn(root, path):\n",
    "    return os.path.join(root, os.path.basename(path))\n",
    "\n",
    "\n",
    "def _filter_fn(split, t):\n",
    "    return Path(t[0]).parts[-3] == split and Path(t[0]).parts[-2] in [\"pos\", \"neg\"]\n",
    "\n",
    "\n",
    "def _file_to_sample(t):\n",
    "    return Path(t[0]).parts[-2], t[1].read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "@_add_docstring_header(num_lines=NUM_LINES, num_classes=2)\n",
    "@_create_dataset_directory(dataset_name=DATASET_NAME)\n",
    "@_wrap_split_argument((\"train\", \"test\"))\n",
    "def IMDB(root, split):\n",
    "    \"\"\"Demonstrates complex use case where each sample is stored in separate file and compressed in tar file\n",
    "    Here we show some fancy filtering and mapping operations.\n",
    "    Filtering is needed to know which files belong to train/test and neg/pos label\n",
    "    Mapping is needed to yield proper data samples by extracting label from file name\n",
    "        and reading data from file\n",
    "    \"\"\"\n",
    "\n",
    "    url_dp = IterableWrapper([URL])\n",
    "    # cache data on-disk\n",
    "    cache_dp = url_dp.on_disk_cache(\n",
    "        filepath_fn=partial(_path_fn, root),\n",
    "        hash_dict={_path_fn(root, URL): MD5},\n",
    "        hash_type=\"md5\",\n",
    "    )\n",
    "    cache_dp = HttpReader(cache_dp).end_caching(mode=\"wb\", same_filepath_fn=True)\n",
    "\n",
    "    cache_dp = FileOpener(cache_dp, mode=\"b\")\n",
    "\n",
    "    # stack TAR extractor on top of load files data pipe\n",
    "    extracted_files = cache_dp.load_from_tar()\n",
    "\n",
    "    # filter the files as applicable to create dataset for given split (train or test)\n",
    "    filter_files = extracted_files.filter(partial(_filter_fn, split))\n",
    "\n",
    "    # map the file to yield proper data samples\n",
    "    sample = filter_files.map(_file_to_sample)\n",
    "    \n",
    "    def convlabel(x):\n",
    "        r = None\n",
    "        if x[0] == 'pos':\n",
    "            r = 1\n",
    "        elif x[0] == 'neg':\n",
    "            r = 0\n",
    "        else:\n",
    "            r = -1\n",
    "            raise(ValueError(f'Error: {x[0]} is not proper value'))\n",
    "        \n",
    "        return r, x[1]\n",
    "    \n",
    "    # sample = sample.map(lambda x: (1 if x[0]=='pos' else 0, x[1]))\n",
    "    sample = sample.map(convlabel)\n",
    "    # sample = sample.shuffle().set_shuffle(False).sharding_filter()\n",
    "    sample = sample.shuffle().sharding_filter()\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, \"This movie had a good story, but was brought down because it didn't have enough horror film elements and violence. It was like watching a live action cartoon. It would of been better if this story is what they planned from the start of the first movie so they could of played seeds for where the series was going.\")\n"
     ]
    }
   ],
   "source": [
    "SPLIT_SIZE = 0.2\n",
    "\n",
    "train_iter = IMDB(split='train')\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = get_tokenizer('spacy', 'en_core_web_md')\n",
    "\n",
    "print(next(iter(train_iter)))\n",
    "count = 0\n",
    "for label, text in train_iter:\n",
    "    if label == 1:\n",
    "        count += 1\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e45d430dd4495a451adf3f96c36ab39ddd21d42ca8131a0e0d50ee113b976380"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
