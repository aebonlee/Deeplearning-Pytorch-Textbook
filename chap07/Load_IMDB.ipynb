{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCPWJN8cqAx-",
        "outputId": "6e5f3df6-213d-43be-a592-79b7267b9067"
      },
      "outputs": [],
      "source": [
        "#!pip install torchtext torchdata datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "x5sF6gJDIA_t"
      },
      "outputs": [],
      "source": [
        "# # Using TPU\n",
        "# !pip install torchtext torchdata cloud-tpu-client==0.10 torch==1.12.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.12-cp37-cp37m-linux_x86_64.whl\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# device = xm.xla_device()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-zj8XSYp7LV",
        "outputId": "0a008207-03de-4169-fce9-ca3e0fa86931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "selected device: cuda:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reusing dataset imdb (C:\\Users\\sms20\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e598584d7b5402788ff12ee53261e38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "571a01d894f045fb959d1934c10d705c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0ex [00:00, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "Provided `function` which is applied to all elements of table returns a variable of type <class 'list'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32md:\\Timepercent\\AI Learning\\Deeplearning-Pytorch-Textbook\\chap07\\Load_IMDB.ipynb 셀 3\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/Load_IMDB.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [tok\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m tok \u001b[39min\u001b[39;00m spacy_en\u001b[39m.\u001b[39mtokenizer(example[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m])]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/Load_IMDB.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m#train_data = train_data.map(tokenize_data, fn_kwargs={'tokenizer': tokenizer})\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/Load_IMDB.ipynb#W2sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m#test_data = test_data.map(tokenize_data, fn_kwargs={'tokenizer': tokenizer})\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/Load_IMDB.ipynb#W2sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m train_data \u001b[39m=\u001b[39m train_data\u001b[39m.\u001b[39;49mmap(tokenize)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/Load_IMDB.ipynb#W2sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m test_data \u001b[39m=\u001b[39m test_data\u001b[39m.\u001b[39mmap(tokenize)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/Load_IMDB.ipynb#W2sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(train_data, test_data)\n",
            "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\datasets\\arrow_dataset.py:2109\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2106\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(logging\u001b[39m.\u001b[39mget_verbosity() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m utils\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[0;32m   2108\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> 2109\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[0;32m   2110\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m   2111\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m   2112\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m   2113\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m   2114\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m   2115\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2116\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m   2117\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m   2118\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m   2119\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m   2120\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[0;32m   2121\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m   2122\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m   2123\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m   2124\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m   2125\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[0;32m   2126\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[0;32m   2127\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m   2128\u001b[0m     )\n\u001b[0;32m   2129\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2131\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
            "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\datasets\\arrow_dataset.py:518\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    517\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 518\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    519\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    520\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    521\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\datasets\\arrow_dataset.py:485\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    479\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    480\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    481\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    482\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    483\u001b[0m }\n\u001b[0;32m    484\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 485\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    486\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    487\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\datasets\\fingerprint.py:413\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[0;32m    408\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[0;32m    409\u001b[0m             )\n\u001b[0;32m    411\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 413\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    415\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\datasets\\arrow_dataset.py:2467\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   2465\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched:\n\u001b[0;32m   2466\u001b[0m     \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(pbar):\n\u001b[1;32m-> 2467\u001b[0m         example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[0;32m   2468\u001b[0m         \u001b[39mif\u001b[39;00m update_data:\n\u001b[0;32m   2469\u001b[0m             \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\datasets\\arrow_dataset.py:2378\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   2375\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2376\u001b[0m     \u001b[39m# Check if the function returns updated examples\u001b[39;00m\n\u001b[0;32m   2377\u001b[0m     update_data \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable))\n\u001b[1;32m-> 2378\u001b[0m     validate_function_output(processed_inputs, indices)\n\u001b[0;32m   2379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m update_data:\n\u001b[0;32m   2380\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m  \u001b[39m# Nothing to update, let's move on\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\datasets\\arrow_dataset.py:2348\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.validate_function_output\u001b[1;34m(processed_inputs, indices)\u001b[0m\n\u001b[0;32m   2346\u001b[0m \u001b[39m\"\"\"Validate output of the map function.\"\"\"\u001b[39;00m\n\u001b[0;32m   2347\u001b[0m \u001b[39mif\u001b[39;00m processed_inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable)):\n\u001b[1;32m-> 2348\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   2349\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProvided `function` which is applied to all elements of table returns a variable of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(processed_inputs)\u001b[39m}\u001b[39;00m\u001b[39m. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2350\u001b[0m     )\n\u001b[0;32m   2351\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(indices, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, Mapping):\n\u001b[0;32m   2352\u001b[0m     allowed_batch_return_types \u001b[39m=\u001b[39m (\u001b[39mlist\u001b[39m, np\u001b[39m.\u001b[39mndarray)\n",
            "\u001b[1;31mTypeError\u001b[0m: Provided `function` which is applied to all elements of table returns a variable of type <class 'list'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects."
          ]
        }
      ],
      "source": [
        "import functools\n",
        "import datasets\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torchtext.vocab import Vocab\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import spacy\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "spacy_en = spacy.load('en_core_web_md')\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'selected device: {device}')\n",
        "tokenizer = get_tokenizer('spacy', 'en_core_web_md')\n",
        "train_data, test_data = datasets.load_dataset('imdb', split=['train', 'test'])\n",
        "\n",
        "def tokenize_data(example, tokenizer):\n",
        "    tokens = {'tokens': tokenizer(example['text'])}\n",
        "    return tokens\n",
        " \n",
        "def tokenize(example):\n",
        "    tokens = {'tokens': spacy_en.tokenizer(example['text'])}\n",
        "    return tokens\n",
        "\n",
        "#train_data = train_data.map(tokenize_data, fn_kwargs={'tokenizer': tokenizer})\n",
        "#test_data = test_data.map(tokenize_data, fn_kwargs={'tokenizer': tokenizer})\n",
        "train_data = train_data.map(tokenize)\n",
        "test_data = test_data.map(tokenize)\n",
        "print(train_data, test_data)\n",
        "\n",
        "test_size = 0.2\n",
        "train_valid_data = train_data.train_test_split(test_size=test_size)\n",
        "train_data = train_valid_data['train']\n",
        "valid_data = train_valid_data['test']\n",
        "\n",
        "min_freq = 5\n",
        "special_tokens = ['<unk>', '<pad>']\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    train_data['tokens'], min_freq=min_freq, specials=special_tokens\n",
        ")\n",
        "\n",
        "print(vocab(['here', 'is', 'an', 'example']))\n",
        "\n",
        "unk_index = vocab['<unk>']\n",
        "pad_index = vocab['<pad>']\n",
        "vocab.set_default_index(unk_index)\n",
        "\n",
        "\n",
        "text_pipeline = lambda x: vocab(tokenize_data(x))\n",
        "label_pipeline = lambda x: 1 if x == 'pos' else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "y5SIprT1sKg-"
      },
      "outputs": [],
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "        label_list.append(label_pipeline(_label))\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        offsets.append(processed_text.size(0))\n",
        "\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i0JtWY6Fug5m"
      },
      "outputs": [],
      "source": [
        "# Refill Generators & Put in the DataLoader\n",
        "train_iter, test_iter = AG_NEWS(split=('train', 'test'))\n",
        "BATCH_SIZE = 100\n",
        "dataloader = DataLoader(\n",
        "    train_iter, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eDxQRnwZw7Ee"
      },
      "outputs": [],
      "source": [
        "class TextClassificationModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QOYB43CT5O53"
      },
      "outputs": [],
      "source": [
        "train_iter = AG_NEWS(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "W62DZhFC8ND2"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 12  # epoch\n",
        "LR = 4.8  # learning rate\n",
        "# BATCH_SIZE = 64\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = AG_NEWS(split=('train', 'test'))\n",
        "\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = random_split(\n",
        "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "valid_dataloader = DataLoader(\n",
        "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "99qfij4G-OZS"
      },
      "outputs": [],
      "source": [
        "epoch_: int = None\n",
        "\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predicted_label = model(text, offsets)\n",
        "        loss = criterion(predicted_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(\n",
        "                '| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                '| accuracy {:8.3f}'.format(\n",
        "                    epoch, idx, len(dataloader), total_acc / total_count\n",
        "                )\n",
        "            )\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predicted_label = model(text, offsets)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "\n",
        "    return total_acc / total_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw430ju_-SuJ",
        "outputId": "be41fe77-54b1-472d-f916-538aa9b10082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |   500/ 1140 batches | accuracy    0.693\n",
            "| epoch   1 |  1000/ 1140 batches | accuracy    0.862\n",
            "| end of epoch   1 | time:  8.63s | valid accuracy    0.886\n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 1140 batches | accuracy    0.891\n",
            "| epoch   2 |  1000/ 1140 batches | accuracy    0.899\n",
            "| end of epoch   2 | time:  7.10s | valid accuracy    0.897\n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 1140 batches | accuracy    0.909\n",
            "| epoch   3 |  1000/ 1140 batches | accuracy    0.911\n",
            "| end of epoch   3 | time:  7.19s | valid accuracy    0.907\n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 1140 batches | accuracy    0.919\n",
            "| epoch   4 |  1000/ 1140 batches | accuracy    0.918\n",
            "| end of epoch   4 | time:  7.10s | valid accuracy    0.911\n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 1140 batches | accuracy    0.925\n",
            "| epoch   5 |  1000/ 1140 batches | accuracy    0.924\n",
            "| end of epoch   5 | time:  5.83s | valid accuracy    0.905\n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   500/ 1140 batches | accuracy    0.935\n",
            "| epoch   6 |  1000/ 1140 batches | accuracy    0.938\n",
            "| end of epoch   6 | time:  5.41s | valid accuracy    0.918\n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   500/ 1140 batches | accuracy    0.938\n",
            "| epoch   7 |  1000/ 1140 batches | accuracy    0.937\n",
            "| end of epoch   7 | time:  5.20s | valid accuracy    0.917\n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   500/ 1140 batches | accuracy    0.940\n",
            "| epoch   8 |  1000/ 1140 batches | accuracy    0.938\n",
            "| end of epoch   8 | time:  5.29s | valid accuracy    0.918\n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   500/ 1140 batches | accuracy    0.937\n",
            "| epoch   9 |  1000/ 1140 batches | accuracy    0.940\n",
            "| end of epoch   9 | time:  5.31s | valid accuracy    0.918\n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   500/ 1140 batches | accuracy    0.939\n",
            "| epoch  10 |  1000/ 1140 batches | accuracy    0.941\n",
            "| end of epoch  10 | time:  5.34s | valid accuracy    0.918\n",
            "-----------------------------------------------------------\n",
            "| epoch  11 |   500/ 1140 batches | accuracy    0.939\n",
            "| epoch  11 |  1000/ 1140 batches | accuracy    0.939\n",
            "| end of epoch  11 | time:  5.42s | valid accuracy    0.918\n",
            "-----------------------------------------------------------\n",
            "| epoch  12 |   500/ 1140 batches | accuracy    0.940\n",
            "| epoch  12 |  1000/ 1140 batches | accuracy    0.938\n",
            "| end of epoch  12 | time:  5.67s | valid accuracy    0.918\n",
            "-----------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_ = epoch\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "        scheduler.step()\n",
        "    else:\n",
        "        total_accu = accu_val\n",
        "\n",
        "    print(\n",
        "        '| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "        'valid accuracy {:8.3f}'.format(epoch, time.time() - epoch_start_time, accu_val)\n",
        "    )\n",
        "    print('-' * 59)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Cx7U_VSzXlLs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.906\n"
          ]
        }
      ],
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nhgxJsINXl17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is a Sports news\n"
          ]
        }
      ],
      "source": [
        "IMDB_label_back = {0: \"neg\", 1: \"pos\"}\n",
        "\n",
        "\n",
        "def predict(text, text_pipeline):\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor(text_pipeline(text))\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "model = model.to('cpu')\n",
        "\n",
        "print(f\"This is a {IMDB_label_back[predict(ex_text_str, text_pipeline)]} mood\")\n",
        "\n",
        "en_text_str = \"\"\"I'm struggling to finish this\n",
        "\n",
        "I'm 6 episodes in and this series feels kind of off. Having read the comics, the characters don't seem like theyre the same personalities. It's very slow, and the dialogue is terrible. The only thing keeping me watching at this point is a hope that I can see more of the endless (especially delirium).\n",
        "Netflix should stay away from making anymore adaptations (especially after the cowboy bebop flop). I really think Gaiman made a mistake choosing Netflix, and I hope to live long enough to see another company remake it.\n",
        "Ill update my review if the show is any better once I finish.\"\"\"\n",
        "\n",
        "print(f\"This is a {IMDB_label_back[predict(ex_text_str, text_pipeline)]} mood\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Load_IMDB.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "e45d430dd4495a451adf3f96c36ab39ddd21d42ca8131a0e0d50ee113b976380"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
