{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCPWJN8cqAx-",
        "outputId": "6e5f3df6-213d-43be-a592-79b7267b9067"
      },
      "outputs": [],
      "source": [
        "#!pip install torchtext torchdata datasets spacy\n",
        "#!python -m spacy download en_core_web_md\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "x5sF6gJDIA_t"
      },
      "outputs": [],
      "source": [
        "# # Using TPU\n",
        "# !pip install torchtext torchdata cloud-tpu-client==0.10 torch==1.12.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.12-cp37-cp37m-linux_x86_64.whl\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# device = xm.xla_device()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-zj8XSYp7LV",
        "outputId": "0a008207-03de-4169-fce9-ca3e0fa86931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "selected device: cuda:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reusing dataset imdb (C:\\Users\\sms20\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49e4c9174011462b975f6a089ef95064",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3258fe094ea4fefbc1b3bb88bae2124",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0ex [00:00, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "308947d4c3454d91b68ae7b5adfcc980",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0ex [00:00, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'label', 'tokens'],\n",
            "    num_rows: 25000\n",
            "}) Dataset({\n",
            "    features: ['text', 'label', 'tokens'],\n",
            "    num_rows: 25000\n",
            "})\n",
            "[163, 9, 43, 490]\n"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'selected device: {device}')\n",
        "tokenizer = get_tokenizer('spacy', 'en_core_web_md')\n",
        "train_data, test_data = datasets.load_dataset('imdb', split=['train', 'test'])\n",
        "\n",
        "\n",
        "def tokenize_data(example, tokenizer):\n",
        "    tokens = {'tokens': tokenizer(example['text'])}\n",
        "    return tokens\n",
        "\n",
        "\n",
        "train_data = train_data.map(tokenize_data, fn_kwargs={'tokenizer': tokenizer})\n",
        "test_data = test_data.map(tokenize_data, fn_kwargs={'tokenizer': tokenizer})\n",
        "print(train_data, test_data)\n",
        "\n",
        "test_size = 0.2\n",
        "train_valid_data = train_data.train_test_split(test_size=test_size)\n",
        "train_data = train_valid_data['train']\n",
        "valid_data = train_valid_data['test']\n",
        "\n",
        "min_freq = 5\n",
        "special_tokens = ['<unk>', '<pad>']\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    train_data['tokens'], min_freq=min_freq, specials=special_tokens\n",
        ")\n",
        "\n",
        "print(vocab(['here', 'is', 'an', 'example']))\n",
        "\n",
        "unk_index = vocab['<unk>']\n",
        "pad_index = vocab['<pad>']\n",
        "vocab.set_default_index(unk_index)\n",
        "\n",
        "text_pipeline = lambda x: vocab(tokenize_data(x))\n",
        "label_pipeline = lambda x: 1 if x == 'pos' else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2a28dfaa97d43f6a19ecaeabe55a5d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0ex [00:00, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95152b96ee8d40a282f261c8e926acb7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0ex [00:00, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79bdef49b9a34b9495c629f28c1a13c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0ex [00:00, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'label': tensor(0), 'ids': tensor([   11,   143,  1226,    17,  1183,     5,   268,  1791,     8,   299,\n",
            "           64,    21,     6,   194,     4,  2556,    17,  1183,   115,   168,\n",
            "          274,    32,   707,     0,     3,   506,    17,  2622,     5,  7408,\n",
            "            4,    11,   814,     8,    47,    16,     3,    11,    75,    86,\n",
            "            3,    26,    12,     9,     8,    60,   274,  1226,    17,  1183,\n",
            "           20, 19468,   699,     9,     8,  1088,  2284,    28,     2,   219,\n",
            "           30,     4, 14145, 31101,     3,   729,  4057,   740,     3,  5474,\n",
            "         4837,     3,  4924,    13,    85,    29,  1106,     2,  1010,     3,\n",
            "            5,  2176,    38,    17,  2079,   120,    69,    33,    36,  3296,\n",
            "           21,     6,    53,  1226,    17,  1183,    53,   973,     4,    28,\n",
            "           11,   172,   311,    67,    32,   173,     7,    31,    54,    67,\n",
            "           44,   119, 19468,   699,     9,    60,  1226,    17,  1183,   274,\n",
            "            4,    55,    15,    33,     4,    55,    15,  2798,     5,  9935,\n",
            "            4,    30,   556,  1193,   841,   254,    47,  1483,     5,   121,\n",
            "          984,     3,  1226,    17,  1183,     9,     6,   553,    13,    85,\n",
            "           33,   217,   430,   787,    28,     0,     4,  1088,  2284,    30,\n",
            "            4,    55,   234,  2062,   708,  1421,     3,   302,    33,    20,\n",
            "            6,   658,  4543,     4,    55,    15,    75,   884,     8,   503,\n",
            "           52,     2,   120,   163,    20,    45,    32,    33,   361,  8141,\n",
            "            3,    56,  1049,     6,  6105,     7,   134,     4,  1764,  1737,\n",
            "            5,  3459,    32,  1841,     5,   755,     3,   440,  1369,     8,\n",
            "          129,     4,    25,  1205,     7,  1275,  8072,    12,    15,  2179,\n",
            "           20,    45,    34,     8,   232,   151,    14,  1921, 14717,    15,\n",
            "         1275,    88,    14,  1195,    98,    63,    33,  1765,   179,     4,\n",
            "        14717,    15, 13655,   230,    36,  1714,    10,    78, 18060,    20,\n",
            "           16,   808,     3,   729,     3,   916,  2135,    28,   179,    12,\n",
            "          242, 21185,  1994,    75,   962,    16,   387,    30,     0,     0,\n",
            "            7,     6,   144,     0,    93,  1030,     4,  9524,     4,   331,\n",
            "            3,   592,   140,     6,   314,   121,     4,   137,   122,   747,\n",
            "           99,   166,    20,   201,   300,     4,     0,    42,  8882,    41,\n",
            "          136,   200,     4])}\n"
          ]
        }
      ],
      "source": [
        "def numericalize_data(example, vocab):\n",
        "    ids = {'ids': [vocab[token] for token in example['tokens']]}\n",
        "    return ids\n",
        "\n",
        "\n",
        "train_data = train_data.map(numericalize_data, fn_kwargs={'vocab': vocab})\n",
        "valid_data = valid_data.map(numericalize_data, fn_kwargs={'vocab': vocab})\n",
        "test_data = test_data.map(numericalize_data, fn_kwargs={'vocab': vocab})\n",
        "\n",
        "train_data.set_format(type='torch', columns=['ids', 'label'])\n",
        "valid_data.set_format(type='torch', columns=['ids', 'label'])\n",
        "test_data.set_format(type='torch', columns=['ids', 'label'])\n",
        "\n",
        "next(iter(test_data))\n",
        "print(next(iter(test_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "i0JtWY6Fug5m"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "LR = 0.0005  # learning rate\n",
        "EPOCHS = 12  # epoch\n",
        "\n",
        "n_classes = 2\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 128  # 각 단어를 128차원으로 조정(임베딩 계층을 통과한 후 각 벡터의 크기)\n",
        "hidden_dim = 256\n",
        "n_layers = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BasicRNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_layers,\n",
        "        hidden_dim,\n",
        "        n_vocab,\n",
        "        embed_dim,\n",
        "        n_classes,\n",
        "        pad_index,\n",
        "        dropout_p=0.2,\n",
        "    ):\n",
        "        super(BasicRNN, self).__init__()\n",
        "        self.n_layers = n_layers  # ------ RNN 계층에 대한 개수\n",
        "        self.embed = nn.Embedding(n_vocab, embed_dim, pad_index)  # ------ 워드 임베딩 적용\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = nn.Dropout(dropout_p)  # ------ 드롭아웃 적용\n",
        "        self.rnn = nn.RNN(\n",
        "            embed_dim, self.hidden_dim, num_layers=self.n_layers, batch_first=True\n",
        "        )\n",
        "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
        "\n",
        "    def forward(self, text):\n",
        "        x = self.embed(text)\n",
        "        h_0 = self._init_state(batch_size=x.size(0))\n",
        "        x, _ = self.rnn(x, h_0)\n",
        "        h_t = x[:, -1, :]\n",
        "        self.dropout(h_t)\n",
        "        logit = torch.sigmoid(self.out(h_t))\n",
        "        return logit\n",
        "\n",
        "    def _init_state(self, batch_size=1):\n",
        "        weight = next(self.parameters()).data  # ------ 모델의 파라미터 값을 가져와서 weight 변수에 저장\n",
        "        return weight.new(\n",
        "            self.n_layers, batch_size, self.hidden_dim\n",
        "        ).zero_()  # ------ 크기가 (계층의 개수, 배치 크기, 은닉층의 뉴런/유닛 개수)인 은닉 상태(텐서)를 생성하여 0으로 초기화한 후 반환\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "99qfij4G-OZS"
      },
      "outputs": [],
      "source": [
        "model = BasicRNN(\n",
        "    n_layers=n_layers,\n",
        "    hidden_dim=hidden_dim,\n",
        "    n_vocab=vocab_size,\n",
        "    embed_dim=embed_dim,\n",
        "    n_classes=n_classes,\n",
        "    pad_index=pad_index,\n",
        "    dropout_p=0.5,\n",
        ").to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)  # loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, gamma=0.1)\n",
        "\n",
        "\n",
        "def collate(batch, pad_index):\n",
        "    batch_ids = [b['ids'] for b in batch]\n",
        "    batch_ids = nn.utils.rnn.pad_sequence(\n",
        "        batch_ids, padding_value=pad_index, batch_first=True\n",
        "    )\n",
        "    batch_label = [b['label'] for b in batch]\n",
        "    batch_label = torch.stack(batch_label)\n",
        "\n",
        "    batch = {'ids': batch_ids, 'label': batch_label}\n",
        "    return batch\n",
        "\n",
        "\n",
        "collate = functools.partial(collate, pad_index=pad_index)\n",
        "\n",
        "# Refill Generators & Put in the DataLoader\n",
        "train_iter, valid_iter, test_iter = train_data, valid_data, test_data\n",
        "\n",
        "train_dataloader, valid_dataloader, test_dataloader = (\n",
        "    DataLoader(train_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate),\n",
        "    DataLoader(valid_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate),\n",
        "    DataLoader(test_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 1.34 GiB (GPU 0; 2.00 GiB total capacity; 395.97 MiB already allocated; 0 bytes free; 1.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32md:\\Timepercent\\AI Learning\\Deeplearning-Pytorch-Textbook\\chap07\\7.4.1_IMDB.ipynb 셀 8\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X21sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m accuracy\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X21sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X21sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(train_dataloader, model, criterion, optimizer, device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X21sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     valid_loss, valid_acc \u001b[39m=\u001b[39m evaluate(valid_dataloader, model, criterion, device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X21sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;32md:\\Timepercent\\AI Learning\\Deeplearning-Pytorch-Textbook\\chap07\\7.4.1_IMDB.ipynb 셀 8\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tokens \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m labels \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m predictions \u001b[39m=\u001b[39m model(tokens)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(predictions, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m accuracy \u001b[39m=\u001b[39m get_accuracy(predictions, labels)\n",
            "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32md:\\Timepercent\\AI Learning\\Deeplearning-Pytorch-Textbook\\chap07\\7.4.1_IMDB.ipynb 셀 8\u001b[0m in \u001b[0;36mBasicRNN.forward\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed(text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m h_0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_state(batch_size\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m x, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x, h_0)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X21sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m h_t \u001b[39m=\u001b[39m x[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X21sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(h_t)\n",
            "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:471\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRNN_TANH\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 471\u001b[0m         result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mrnn_tanh(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    472\u001b[0m                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional,\n\u001b[0;32m    473\u001b[0m                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    474\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    475\u001b[0m         result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mrnn_relu(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    476\u001b[0m                               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional,\n\u001b[0;32m    477\u001b[0m                               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.34 GiB (GPU 0; 2.00 GiB total capacity; 395.97 MiB already allocated; 0 bytes free; 1.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "def train(dataloader, model, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        tokens = batch['ids'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        predictions = model(tokens)\n",
        "        loss = criterion(predictions, labels)\n",
        "        accuracy = get_accuracy(predictions, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_accuracy += accuracy.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader), epoch_accuracy / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate(dataloader, model, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            tokens = batch['ids']\n",
        "            labels = batch['label']\n",
        "            predictions = model(tokens)\n",
        "            loss = criterion(predictions, labels)\n",
        "            accuracy = get_accuracy(predictions, labels)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_accuracy += accuracy.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader), epoch_accuracy / len(dataloader)\n",
        "\n",
        "\n",
        "def get_accuracy(predictions, labels):\n",
        "    batch_size = predictions.shape[0]\n",
        "    predicted_classes = predictions.argmax(1, keepdim=True)\n",
        "    correct_predictions = predicted_classes.eq(labels.view_as(predicted_classes)).sum()\n",
        "    accuracy = correct_predictions.float() / batch_size\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)\n",
        "    valid_loss, valid_acc = evaluate(valid_dataloader, model, criterion, device)\n",
        "\n",
        "    print(f'epoch: {epoch+1}')\n",
        "    print(f'train_loss: {train_loss:.3f}, train_acc: {train_acc:.3f}')\n",
        "    print(f'valid_loss: {valid_loss:.3f}, valid_acc: {valid_acc:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.plot(train_losses, label='train loss')\n",
        "ax.plot(valid_losses, label='valid loss')\n",
        "plt.legend()\n",
        "ax.set_xlabel('updates')\n",
        "ax.set_ylabel('loss')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.plot(train_accs, label='train accuracy')\n",
        "ax.plot(valid_accs, label='valid accuracy')\n",
        "plt.legend()\n",
        "ax.set_xlabel('updates')\n",
        "ax.set_ylabel('accuracy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss, test_acc = evaluate(test_dataloader, model, criterion, device)\n",
        "\n",
        "epoch_test_loss = np.mean(test_loss)\n",
        "epoch_test_acc = np.mean(test_acc)\n",
        "\n",
        "print(f'test_loss: {epoch_test_loss:.3f}, test_acc: {epoch_test_acc:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_sentiment(text, model, tokenizer, vocab, device):\n",
        "    tokens = tokenizer(text)\n",
        "    ids = [vocab[t] for t in tokens]\n",
        "    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n",
        "    prediction = model(tensor).squeeze(dim=0)\n",
        "    probability = torch.softmax(prediction, dim=-1)\n",
        "    predicted_class = prediction.argmax(dim=-1).item()\n",
        "    predicted_probability = probability[predicted_class].item()\n",
        "    return predicted_class, predicted_probability\n",
        "\n",
        "\n",
        "text = \"This film is terrible!\"\n",
        "predict_sentiment(text, model, tokenizer, vocab, device)\n",
        "\n",
        "text = \"This film is great!\"\n",
        "predict_sentiment(text, model, tokenizer, vocab, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhgxJsINXl17"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "For unbatched 2-D input, hx should also be 2-D but got 3-D tensor",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32md:\\Timepercent\\AI Learning\\Deeplearning-Pytorch-Textbook\\chap07\\7.4.1_IMDB.ipynb 셀 9\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m ex_text_str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMEMPHIS, Tenn. – Four days ago, Jon Rahm was \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m    enduring the season’s worst weather conditions on Sunday at The \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m    Open on his way to a closing 75 at Royal Portrush, which \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m    was even more impressive considering he’d never played the \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m    front nine at TPC Southwind.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis is a \u001b[39m\u001b[39m{\u001b[39;00mpredict(ex_text_str, text_pipeline)\u001b[39m}\u001b[39;00m\u001b[39m mood\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m en_text_str \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm struggling to finish this\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mI\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm 6 episodes in and this series feels kind of off. Having read the comics, the characters don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt seem like theyre the same personalities. It\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms very slow, and the dialogue is terrible. The only thing keeping me watching at this point is a hope that I can see more of the endless (especially delirium).\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mNetflix should stay away from making anymore adaptations (especially after the cowboy bebop flop). I really think Gaiman made a mistake choosing Netflix, and I hope to live long enough to see another company remake it.\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mIll update my review if the show is any better once I finish.\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis is a \u001b[39m\u001b[39m{\u001b[39;00mIMDB_label[predict(ex_text_str, text_pipeline)]\u001b[39m}\u001b[39;00m\u001b[39m mood\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;32md:\\Timepercent\\AI Learning\\Deeplearning-Pytorch-Textbook\\chap07\\7.4.1_IMDB.ipynb 셀 9\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(text, text_pipeline)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     text \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(text_pipeline(text))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     output \u001b[39m=\u001b[39m model(text\u001b[39m.\u001b[39;49mflatten())\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m output\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mitem()\n",
            "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32md:\\Timepercent\\AI Learning\\Deeplearning-Pytorch-Textbook\\chap07\\7.4.1_IMDB.ipynb 셀 9\u001b[0m in \u001b[0;36mBasicRNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m h_0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_state(batch_size\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m x, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x, h_0)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m h_t \u001b[39m=\u001b[39m x[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Timepercent/AI%20Learning/Deeplearning-Pytorch-Textbook/chap07/7.4.1_IMDB.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(h_t)\n",
            "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\sms20\\mambaforge\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:445\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    443\u001b[0m     \u001b[39mif\u001b[39;00m hx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    444\u001b[0m         \u001b[39mif\u001b[39;00m hx\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m--> 445\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    446\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFor unbatched 2-D input, hx should also be 2-D but got \u001b[39m\u001b[39m{\u001b[39;00mhx\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D tensor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    447\u001b[0m         hx \u001b[39m=\u001b[39m hx\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m    448\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "\u001b[1;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx should also be 2-D but got 3-D tensor"
          ]
        }
      ],
      "source": [
        "IMDB_label = {0: \"neg\", 1: \"pos\"}\n",
        "\n",
        "\n",
        "def predict(text, text_pipeline):\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor(text_pipeline(text))\n",
        "        output = model(text.view)\n",
        "        return output.argmax(1).item()\n",
        "\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "model = model.to('cpu')\n",
        "\n",
        "print(f\"This is a {predict(ex_text_str, text_pipeline)} mood\")\n",
        "\n",
        "en_text_str = \"\"\"I'm struggling to finish this\n",
        "\n",
        "I'm 6 episodes in and this series feels kind of off. Having read the comics, the characters don't seem like theyre the same personalities. It's very slow, and the dialogue is terrible. The only thing keeping me watching at this point is a hope that I can see more of the endless (especially delirium).\n",
        "Netflix should stay away from making anymore adaptations (especially after the cowboy bebop flop). I really think Gaiman made a mistake choosing Netflix, and I hope to live long enough to see another company remake it.\n",
        "Ill update my review if the show is any better once I finish.\"\"\"\n",
        "\n",
        "print(f\"This is a {IMDB_label[predict(ex_text_str, text_pipeline)]} mood\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CROH9J1PaMkm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Load_IMDB.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "e45d430dd4495a451adf3f96c36ab39ddd21d42ca8131a0e0d50ee113b976380"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
